import os
import re
import tempfile
from typing import List, Optional, Any
import pypdf
import docx
from langchain_community.document_loaders import PyPDFLoader
# Tenta importar o loader com OCR; se n√£o der, segue sem ele (ou avisa)
# rapidocr-onnxruntime e rapidocr-pdf devem estar instalados
try:
    from langchain_community.document_loaders import RapidOCRPDFLoader
    HAS_OCR = True
except ImportError:
    HAS_OCR = False

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
import json
import gc
from prompts import PROMPT_FATOS, PROMPT_ANALISE_FORMAL, PROMPT_ANALISE_MATERIAL, PROMPT_RELATOR_FINAL
from prompts_auditor import PROMPT_AUDITOR_FATICO, PROMPT_AUDITOR_EFICIENCIA, PROMPT_AUDITOR_JURIDICO, PROMPT_AUDITOR_DASHBOARD
from prompts_gemini import PROMPT_GEMINI_INTEGRAL, PROMPT_GEMINI_AUDITOR, PROMPT_STYLE_ANALYZER
try:
    from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
    HAS_GEMINI = True
except ImportError:
    HAS_GEMINI = False


try:
    from mlx_lm import load, generate
    import mlx.core as mx
    HAS_MLX = True
except ImportError:
    HAS_MLX = False

# Registro de Modelos Finetunados (Local)
LOCAL_MODELS = {
    "llama3.1-8b-juris": {
        "model_id": "mlx-community/Meta-Llama-3.1-8B-Instruct-4bit",
        "adapter_path": "adapters/llama3_1_adapter"
    },
    "qwen2.5-14b-juris": {
        "model_id": "mlx-community/Qwen2.5-14B-Instruct-4bit",
        "adapter_path": "adapters/qwen25_14b_adapter"
    },
    "gemma2-9b-juris": {
        "model_id": "mlx-community/gemma-2-9b-it-4bit",
        "adapter_path": "adapters/gemma2_9b_adapter"
    },
    "deepseek-v2-juris": {
        "model_id": "mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit",
        "adapter_path": "adapters/deepseek_v2_lite_adapter"
    },
    "phi3.5-mini-juris": {
        "model_id": "mlx-community/Phi-3.5-mini-instruct-4bit",
        "adapter_path": "adapters/phi3_5_mini_adapter"
    }
}



# Defini√ß√£o de Especialistas (Model Routing)
AGENT_MODEL_MAP = {
    "formal": "phi3.5-mini-juris",       # R√°pido e bom de checklist
    "fatos": "qwen2.5-14b-juris",        # √ìtima compreens√£o de contexto
    "material": "qwen2.5-14b-juris",     # Racioc√≠nio jur√≠dico profundo
    "relator": "gemma2-9b-juris",        # √ìtima escrita/reda√ß√£o
    "auditor": "qwen2.5-14b-juris"       # Cr√≠tico e rigoroso
}

class MLXChatWrapper:
    """Wrapper simples para usar modelos MLX compat√≠vel com a interface invoke do LangChain."""
    def __init__(self, model_key):
        if not HAS_MLX:
            raise ImportError("mlx_lm n√£o est√° instalado.")
        
        self.model_key = model_key
        config = LOCAL_MODELS[model_key]
        print(f"üîÑ Carregando modelo especializado: {model_key}...")
        
        # Carrega modelo e tokenizer
        # Se os adapters n√£o existirem (ex: n√£o treinou ainda), carrega s√≥ o base model
        adapter_file = os.path.join(config['adapter_path'], "adapters.safetensors")
        if os.path.exists(adapter_file):
            self.model, self.tokenizer = load(config['model_id'], adapter_path=config['adapter_path'])
        else:
            print(f"Aviso: Adapter n√£o encontrado em {config['adapter_path']}. Carregando modelo base.")
            self.model, self.tokenizer = load(config['model_id'])
            
    def invoke(self, messages):
        # Converte mensagens LangChain para formato de chat do tokenizer
        # Suporta SystemMessage, HumanMessage, AIMessage
        if "gemma" in self.model_key.lower():
            # Gemma models generally do not support 'system' role. merging into user.
            system_content = ""
            new_messages = []
            for msg in messages:
                if isinstance(msg, SystemMessage):
                    system_content += f"**INSTRU√á√ÉO DO SISTEMA:** {msg.content}\n\n"
                else:
                    new_messages.append(msg)
            
            # Prepend system content to first user message
            if new_messages and isinstance(new_messages[0], HumanMessage):
                new_messages[0].content = system_content + new_messages[0].content
            elif system_content:
                # Fallback if no user message (rare)
                new_messages.insert(0, HumanMessage(content=system_content))
                
            messages = new_messages

        chat_history = []
        for msg in messages:
            role = "user"
            if isinstance(msg, SystemMessage):
                role = "system"
            elif isinstance(msg, AIMessage):
                role = "assistant"
            
            chat_history.append({"role": role, "content": msg.content})
            
        # Aplica template
        try:
           prompt = self.tokenizer.apply_chat_template(chat_history, tokenize=False, add_generation_prompt=True)
        except Exception as e:
           print(f"Erro ao aplicar template (provavelmente role inv√°lida): {e}. Tentando fallback user-only.")
           # Fallback agressivo: apenas user/assistant
           simple_history = []
           for msg in messages:
               role = "user" if isinstance(msg, (SystemMessage, HumanMessage)) else "model"
               simple_history.append({"role": role, "content": msg.content})
           prompt = self.tokenizer.apply_chat_template(simple_history, tokenize=False, add_generation_prompt=True)
        
        # Gera resposta
        response_text = generate(self.model, self.tokenizer, prompt=prompt, max_tokens=2048, verbose=False)
        
        return AIMessage(content=response_text)
    
    def unload(self):
        """Libera mem√≥ria da GPU/RAM."""
        print(f"üóëÔ∏è Descarregando modelo {self.model_key}...")
        del self.model
        del self.tokenizer
        gc.collect()
        if HAS_MLX:
            mx.metal.clear_cache()

def clean_text(text: str) -> str:
    """
    Higieniza√ß√£o agressiva para pe√ßas jur√≠dicas (Otimiza√ß√£o de Context Window).
    Remove: Cabe√ßalhos, Rodap√©s, N√∫meros de P√°gina, Espa√ßos duplos.
    """
    # 1. Normaliza√ß√£o de quebras de linha
    text = text.replace('\r', '')
    
    # 2. Remove cabe√ßalhos de numera√ß√£o de processo (ex: "Processo n¬∫ 1234..." repetido)
    text = re.sub(r'(?i)(fls\.\s*\d+|processo\s*n¬∫?[:\s]*[\d\.\-]+)', '', text)
    
    # 3. Remove rodap√©s de escrit√≥rio/sistema
    # Padr√£o comum: "Rua X, n¬∫ Y... | www.advocacia..." ou "PJe - Assinado eletronicamente"
    text = re.sub(r'(?i)(assinado\s+eletronicamente|documento\s+assinado|pje).*', '', text) 
    
    # 4. Remove n√∫meros de p√°gina soltos
    text = re.sub(r'\n\s*\d+\s*\n', '\n', text)
    
    # 5. Redu√ß√£o de ru√≠do visual (tra√ßos, asteriscos)
    text = re.sub(r'[_=\-\*]{3,}', '', text)
    
    # 6. Compress√£o de espa√ßos (White space normalization)
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def get_embedding_function(api_key=None):
    # Detecta tipo de chave
    if api_key:
        if api_key.startswith("AIza"):
            if HAS_GEMINI:
                return GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=api_key)
            else:
                print("‚ö†Ô∏è Chave Google detectada mas lib n√£o instalada. Usando local.")
        elif api_key.startswith("sk-"):
            return OpenAIEmbeddings(openai_api_key=api_key)
            
    # Modelo leve para rodar localmente no Mac M3
    print("‚ö†Ô∏è Usando Embeddings Locais (HuggingFace)...")
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

def process_uploaded_file(file_obj, filename: str, api_key=None):
    """
    Salva arquivo temp, faz OCR se necess√°rio, vetoriza e retorna (full_text, retriever).
    """
    text = ""
    docs = []
    
    # Cria arquivo tempor√°rio para processamento (necess√°rio para loaders do Langchain)
    suffix = os.path.splitext(filename)[1].lower()
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
        tmp_file.write(file_obj.read())
        tmp_path = tmp_file.name

    try:
        if suffix == ".pdf":
            # 1. Tenta extra√ß√£o padr√£o r√°pida
            loader = PyPDFLoader(tmp_path)
            docs = loader.load()
            
            # Verifica se extraiu texto suficiente
            # Documentos PDFs digitalizados como imagem retornam pouqu√≠ssimo texto (s√≥ metadados)
            # Aumentei o threshold para 500 chars para ser mais seguro em docs grandes
            total_chars = sum(len(d.page_content) for d in docs)
            
            # 2. Se falhar (PDF escaneado/imagem), tenta OCR
            if total_chars < 500:
                if HAS_OCR:
                    print(f"Detectado PDF imagem (apenas {total_chars} chars). Iniciando OCR...")
                    loader_ocr = RapidOCRPDFLoader(tmp_path)
                    docs = loader_ocr.load()
                else:
                    text += "[AVISO: PDF parece ser imagem e biblioteca de OCR n√£o encontrada.]\n"
        
        elif suffix == ".docx":
            from langchain_community.document_loaders import Docx2txtLoader
            loader = Docx2txtLoader(tmp_path)
            docs = loader.load()
            
        elif suffix == ".txt":
            from langchain_community.document_loaders import TextLoader
            loader = TextLoader(tmp_path, encoding='utf-8')
            docs = loader.load()
            
        else:
            return f"Formato n√£o suportado: {filename}", None

        # Limpeza e Consolida√ß√£o
        full_text = ""
        for doc in docs:
            cleaned = clean_text(doc.page_content)
            doc.page_content = cleaned
            full_text += cleaned + "\n\n"
            
        print(f"Texto extra√≠do: {len(full_text)} caracteres.")

        # Vetoriza√ß√£o (RAG)
        # Divide em chunks
        if not docs:
             return "Nenhum texto extra√≠do.", None

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )
        splits = text_splitter.split_documents(docs)
        
        # Cria Vector Store em mem√≥ria (ou temp dir que apagamos depois)
        # Usando Chroma
        embedding_function = get_embedding_function(api_key=api_key)
        
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embedding_function,
            collection_name="temp_process_analysis" 
            # N√£o definimos persist_directory para ser in-memory (ephemeral)
        )
        
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        
        return full_text, retriever
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"Erro ao processar arquivo: {str(e)}", None
    finally:
        # Limpa arquivo tempor√°rio
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

def get_llm(model_name: str, api_key: str = None, temperature: float = 0.1):
    """
    Retorna a inst√¢ncia do LLM (Ollama ou OpenAI).
    """
    if "gpt" in model_name.lower():
        if not api_key:
            # Fallback or error is better handled in UI, but raising here is safe
            raise ValueError("API Key √© obrigat√≥ria para modelos GPT.")
        return ChatOpenAI(api_key=api_key, model=model_name, temperature=temperature)
    
    # Configura√ß√£o para modelos locais via Ollama ou MLX
    
    # 1. Verifica se √© um modelo MLX registrado
    if model_name in LOCAL_MODELS:
        # Retorna o wrapper MLX
        # Nota: O ideal seria cachear isso no n√≠vel da aplica√ß√£o (app.py) para n√£o recarregar pesos
        # Mas instanciamos aqui para manter a assinatura.
        return MLXChatWrapper(model_name)

    # 2. Tenta conectar ao host local padr√£o (Ollama)
    return ChatOllama(model=model_name, temperature=temperature, base_url="http://localhost:11434")

def run_orchestration(text: str, model_mode: str = "auto", api_key: str = None, status_callback=None):
    """
    Executa o pipeline multi-agente com ROUTING DE MODELOS.
    :param model_mode: "auto" (usa mapa de especialistas) ou nome de modelo √∫nico.
    """
    
    # Cache local de sess√£o para evitar recarregar o mesmo modelo seguidamente
    current_llm = None
    current_model_key = None
    
    def get_agent_llm(role):
        nonlocal current_llm, current_model_key
        
        target_model = AGENT_MODEL_MAP.get(role, "qwen2.5-14b-juris") if model_mode == "auto" else model_mode
        
        # Se for modelo GPT/Ollama (n√£o MLX), usa o get_llm padr√£o
        if target_model not in LOCAL_MODELS:
            return get_llm(target_model, api_key)
            
        # Se j√° estamos com o modelo certo carregado, retorna ele
        if current_llm and current_model_key == target_model:
            return current_llm
            
        # Se precisamos trocar de modelo
        if current_llm and hasattr(current_llm, "unload"):
            current_llm.unload()
            
        if status_callback:
            status_callback(f"üîÑ Carregando Especialista: {target_model}...")
            
        current_llm = MLXChatWrapper(target_model)
        current_model_key = target_model
        return current_llm

    # helper para invocar
    def invoke_agent(system_prompt, user_content, agent_name, role_key):
        if status_callback:
            model_name = current_model_key if current_model_key else model_mode
            status_callback(f"ü§ñ {agent_name} trabalhando... (Model: {model_name})")
        
        llm = get_agent_llm(role_key)
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_content)
        ]
        response = llm.invoke(messages)
        return response.content

    try:
        # --- PASSO 1: AN√ÅLISE FORMAL (Phi-3.5 - R√°pido) ---
        formal_out = invoke_agent(PROMPT_ANALISE_FORMAL, f"Analise formalmente a peti√ß√£o:\n\n{text[:50000]}", "Agente de An√°lise Formal", "formal")
        
        # --- PASSO 2: FATOS (Qwen 14B - Contexto) ---
        fatos_out = invoke_agent(PROMPT_FATOS, f"Extraia os dados b√°sicos deste processo:\n\n{text[:50000]}", "Agente de Fatos", "fatos")
        
        # --- PASSO 3: MATERIAL/TEMPORAL (Qwen 14B - Racioc√≠nio) ---
        # Note: Fatos e Material usam o mesmo modelo, ent√£o n√£o haver√° reload aqui
        material_out = invoke_agent(PROMPT_ANALISE_MATERIAL, f"Analise m√©rito liminar, prescri√ß√£o e in√©pcia:\n\n{text[:50000]}\n\nFatos extra√≠dos: {fatos_out}", "Agente de Admissibilidade", "material")
        
        # --- PASSO 4: RELATOR (Gemma 9B - Escrita) ---
        relator_input = PROMPT_RELATOR_FINAL.format(
            fatos_texto=fatos_out,
            formal_json=formal_out,
            material_texto=material_out
        )
        final_report = invoke_agent(relator_input, "Gere o Relat√≥rio de Triagem Final.", "Agente Relator/Chefe de Gabinete", "relator")

        # --- PASSO 5: AUDITOR (Qwen 14B - Review) ---
        # 5.1 Auditor F√°tico
        auditor_fatico_out = invoke_agent(PROMPT_AUDITOR_FATICO.format(fatos_originais=fatos_out, minuta_gerada=final_report), "Valide integridade f√°tica.", "Auditor de Conformidade (Fatos)", "auditor")
        
        # 5.2 Auditor Efici√™ncia
        auditor_eficiencia_out = invoke_agent(PROMPT_AUDITOR_EFICIENCIA.format(minuta_gerada=final_report), "Valide efici√™ncia (Prov. 355).", "Auditor de Conformidade (Efici√™ncia)", "auditor")
        
        # 5.3 Auditor Jur√≠dico
        auditor_juridico_out = invoke_agent(PROMPT_AUDITOR_JURIDICO.format(pedidos_iniciais=fatos_out, minuta_gerada=final_report), "Valide congru√™ncia jur√≠dica.", "Auditor de Conformidade (Jur√≠dico)", "auditor")
        
        # 5.4 Dashboard
        dashboard_out = invoke_agent(PROMPT_AUDITOR_DASHBOARD.format(
            status_fatico=auditor_fatico_out,
            status_eficiencia=auditor_eficiencia_out,
            status_juridico=auditor_juridico_out
        ), "Gere o Dashboard final.", "Gerador de Dashboard", "auditor")
        
        return {
            "final_report": final_report,
            "auditor_dashboard": dashboard_out,
            "steps": {
                "fatos": fatos_out,
                "formal": formal_out, 
                "material": material_out,
                "auditor_fatico": auditor_fatico_out,
                "auditor_eficiencia": auditor_eficiencia_out,
                "auditor_juridico": auditor_juridico_out
            }
        }
    finally:
        # Limpeza final de mem√≥ria
        if current_llm and hasattr(current_llm, "unload"):
            current_llm.unload()

def process_templates(files, api_key):
    """
    Processa arquivos de template (PDF/DOCX/TXT) e cria um retriever.
    """
    documents = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

    for file in files:
        # Salva temporariamente para processar
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file.name.split('.')[-1]}") as tmp:
            tmp.write(file.read())
            tmp_path = tmp.name
        
        try:
            text = ""
            if file.name.endswith(".pdf"):
                reader = pypdf.PdfReader(tmp_path)
                for page in reader.pages:
                    text += page.extract_text() + "\n"
            elif file.name.endswith(".docx"):
                doc = docx.Document(tmp_path)
                text = "\n".join([p.text for p in doc.paragraphs])
            else: # txt
                with open(tmp_path, "r") as f:
                    text = f.read()
            
            # Adiciona metadados
            doc_chunks = text_splitter.create_documents([text], metadatas=[{"source": file.name}])
            documents.extend(doc_chunks)
        finally:
            os.remove(tmp_path)
    
    if not documents:
        return None

    # Embeddings e Vector Store (Chroma na mem√≥ria)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=api_key)
    vectorstore = Chroma.from_documents(documents, embeddings)
    return vectorstore.as_retriever(search_kwargs={"k": 2}), documents

def generate_style_report(documents, api_key):
    """
    Usa um modelo r√°pido (Flash) para ler os templates e criar um perfil de estilo.
    """
    try:
        # Usa Gemini 2.0 Flash (se dispon√≠vel) ou 1.5 Flash para ser r√°pido
        llm_flash = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=api_key, temperature=0.3)
        
        # Concatena amostras dos documentos (m√°x 30k chars para n√£o gastar muito)
        sample_text = ""
        for doc in documents[:5]: # Pega primeiros 5 chunks
            sample_text += f"\n--- AMOSTRA ({doc.metadata.get('source')}): ---\n{doc.page_content[:5000]}\n"
            
        messages = [
            SystemMessage(content=PROMPT_STYLE_ANALYZER),
            HumanMessage(content=f"Aqui est√£o amostras de decis√µes do magistrado. Crie o Dossi√™ de Estilo:\n{sample_text}")
        ]
        
        return llm_flash.invoke(messages).content
    except Exception as e:
        return f"Erro ao gerar perfil de estilo: {str(e)}"

def run_gemini_orchestration(text: str, api_key: str, status_callback=None, template_files=None):
    """
    Pipeline PROFUNDO usando Gemini 3.0 Pro ou Flash.
    Segue a sequ√™ncia complexa do usu√°rio: An√°lise Integral -> Auditoria.
    Suporta RAG (Retrieval Augmented Generation) se templates forem fornecidos.
    """
    if not HAS_GEMINI:
        return {"final_report": "Erro: Pacote langchain-google-genai n√£o instalado.", "steps": {}}
    
    if not api_key:
        return {"final_report": "Erro: API Key do Google n√£o fornecida.", "steps": {}}

    # Instancia Gemini (modelo robusto para an√°lise profunda)
    # Trocando para gemini-3-pro-preview (Solicitado pelo usu√°rio)
    llm = ChatGoogleGenerativeAI(model="gemini-3-pro-preview", google_api_key=api_key, temperature=0.2)
    
    def update(msg):
        if status_callback:
            status_callback(msg)

    # PROCESSAMENTO DE TEMPLATES (RAG)
    rag_context = ""
    if template_files:
        update("üìö Indexando Modelos de Refer√™ncia (RAG)...")
        try:
            retriever, all_docs = process_templates(template_files, api_key)
            if retriever:
                # 1. RAG (Busca por similaridade)
                relevant_docs = retriever.invoke(text[:4000])
                rag_context = "\n\n## MODELOS DE REFER√äNCIA (RAG)\n"
                rag_context += "Use o ESTILO e ESTRUTURA visual destes modelos:\n"
                for i, doc in enumerate(relevant_docs):
                    rag_context += f"\n[MODELO {i+1} - {doc.metadata.get('source')}]:\n{doc.page_content}\n"
                
                # 2. STYLE ANALYZER (Flash)
                update("üé® Analisando Estilo Judicial (Profiling com Gemini Flash)...")
                style_report = generate_style_report(all_docs, api_key)
                if style_report:
                    rag_context += f"\n\n## DIRETRIZES DE PERSONALIDADE (PERFIL DO JULGADOR)\nVoc√™ deve emular estritamente o seguinte perfil:\n{style_report}\n"

        except Exception as e:
            update(f"‚ö†Ô∏è Erro ao processar modelos: {e}")

    update("üß† Iniciando An√°lise Profunda (Gemini 3.0 Pro)...")

    # 1. AN√ÅLISE INTEGRAL (M√âRITO/MINUTA)
    update("‚öñÔ∏è Fase 1: An√°lise Integral e Minutagem (Analista S√™nior)...")
    
    # Injeta contexto RAG no prompt se houver
    final_prompt_integral = PROMPT_GEMINI_INTEGRAL
    if rag_context:
        final_prompt_integral += rag_context

    integral_messages = [
        SystemMessage(content=final_prompt_integral),
        HumanMessage(content=f"Realize a AN√ÅLISE INTEGRAL E MINUTAGEM deste processo:\n\n[AUTOS DO PROCESSO]: {text[:150000]}") # Aumentado context
    ]
    integral_response = llm.invoke(integral_messages).content
    
    # 2. REVISOR (AUDITOR)
    update("üõ°Ô∏è Fase 2: Auditoria Final (Raio-X)...")
    auditor_messages = [
        SystemMessage(content=PROMPT_GEMINI_AUDITOR),
        HumanMessage(content=f"Audite a Minuta abaixo com base nos autos:\n\n[DADOS DOS AUTOS]: {text[:150000]}\n\n[MINUTA A SER AUDITADA]: {integral_response}")
    ]
    auditor_response = llm.invoke(auditor_messages).content
    
    # Consolida tudo
    final_output = f"""
# üß† RELAT√ìRIO DE AN√ÅLISE PROFUNDA (GEMINI 3.0)

---
## 1. PARECER JUR√çDICO E MINUTA
{integral_response}

---
## 2. AUDITORIA DE CONFORMIDADE
{auditor_response}
    """
    
    return {
        "final_report": final_output,
        "auditor_dashboard": auditor_response,
        "style_report": style_report if 'style_report' in locals() else None,
        "steps": {
            "integral": integral_response,
            "auditor": auditor_response
        }
    }
