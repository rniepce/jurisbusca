# Config for Llama 3.1 8B (Debug / Fast)
model: "mlx-community/Meta-Llama-3.1-8B-Instruct-4bit"
train: true
data: "data"
seed: 42
lora_layers: 4
batch_size: 4
iters: 20
val_batches: 2
learning_rate: 1e-5
steps_per_report: 5
steps_per_eval: 10
adapter_path: "adapters/llama3_1_debug"
save_every: 20
grad_checkpoint: false
max_seq_length: 2048
lora_parameters:
  rank: 4
  scale: 4.0
  dropout: 0.05
