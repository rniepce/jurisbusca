# Config for Llama 3.1 8B (4-bit Quantized) - Stable
model: "mlx-community/Meta-Llama-3.1-8B-Instruct-4bit"
train: true
data: "data"
seed: 42
lora_layers: 4
batch_size: 1
iters: 600
val_batches: 5
learning_rate: 1e-5
steps_per_report: 10
steps_per_eval: 200
adapter_path: "adapters/llama3_1_adapter"
save_every: 200
grad_checkpoint: true
max_seq_length: 1024
lora_parameters:
  rank: 4
  scale: 4.0
  dropout: 0.05
