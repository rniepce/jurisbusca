# Config for DeepSeek Coder V2 Lite Instruct (4-bit Quantized)
# DeepSeek's powerful MoE model (16B total, ~2.4B active params)
model: "mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit"
train: true
data: "data"
seed: 42
lora_layers: 4
batch_size: 1
iters: 600
val_batches: 5
learning_rate: 1e-5
steps_per_report: 10
steps_per_eval: 200
adapter_path: "adapters/deepseek_v2_lite_adapter"
save_every: 200
grad_checkpoint: true
max_seq_length: 1024
lora_parameters:
  rank: 8
  scale: 16.0
  dropout: 0.05
