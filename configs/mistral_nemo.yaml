# Config for Mistral Nemo 12B (4-bit Quantized) - Stable/Truncated
model: "mlx-community/Mistral-Nemo-Instruct-2407-4bit"
train: true
data: "data"
seed: 42
lora_layers: 4
batch_size: 1
iters: 600
val_batches: 5
learning_rate: 1e-5
steps_per_report: 10
steps_per_eval: 200
adapter_path: "adapters/mistral_nemo_adapter"
save_every: 200
grad_checkpoint: true
max_seq_length: 1024
lora_parameters:
  rank: 4
  scale: 4.0
  dropout: 0.05
