import os
import time
import logging
import requests
import numpy as np
from typing import List, Optional
from sqlmodel import Field, Session, SQLModel, create_engine, select, text
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans

# --- Configura√ß√£o de Logging ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("legal_etl.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- Configura√ß√£o do Banco de Dados ---
# Tenta pegar da ENV, sen√£o pede input (seguran√ßa)
DATABASE_URL = os.environ.get("DATABASE_URL")
if not DATABASE_URL:
    logger.warning("DATABASE_URL n√£o encontrada nas vari√°veis de ambiente.")
    # Exemplo: DATABASE_URL = "postgresql://postgres:password@roundhouse.proxy.rlwy.net:12345/railway"

# --- Defini√ß√£o do Modelo (Schema) ---
class Processo(SQLModel, table=True):
    __tablename__ = "processos_juridicos"

    id: Optional[int] = Field(default=None, primary_key=True)
    numero_processo: str = Field(index=True)
    texto_decisao: str
    
    # Campo vetorial para pgvector (384 dimensoes para all-MiniLM-L6-v2)
    embedding: Optional[List[float]] = Field(default=None, sa_column_kwargs={"type_": "vector(384)"})
    
    cluster_id: Optional[int] = Field(default=None)
    sugestao_ia: Optional[str] = Field(default=None)
    
    processado: bool = Field(default=False)
    data_processamento: Optional[str] = Field(default=None)

# --- Classes de Servi√ßo ---

class VectorizerService:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.device = 'mps' if self._check_mps_available() else 'cpu'
        logger.info(f"üöÄ Inicializando modelo de vetoriza√ß√£o em: {self.device.upper()}")
        try:
            self.model = SentenceTransformer(model_name, device=self.device)
        except Exception as e:
            logger.error(f"Erro ao carregar modelo: {e}. Tentando CPU.")
            self.model = SentenceTransformer(model_name, device='cpu')

    def _check_mps_available(self):
        try:
            import torch
            return torch.backends.mps.is_available()
        except ImportError:
            return False

    def generate_embeddings(self, texts: List[str]) -> np.ndarray:
        """Gera embeddings em batch usando a GPU se dispon√≠vel."""
        start_time = time.time()
        embeddings = self.model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
        elapsed = time.time() - start_time
        logger.info(f"‚ö° Vetoriza√ß√£o de {len(texts)} itens conclu√≠da em {elapsed:.2f}s")
        return embeddings

class OllamaService:
    def __init__(self, model="llama3", base_url="http://localhost:11434"):
        self.model = model
        self.base_url = base_url
        self._check_connection()

    def _check_connection(self):
        try:
            requests.get(self.base_url)
            logger.info(f"üü¢ Conectado ao Ollama em {self.base_url}")
        except Exception:
            logger.error(f"üî¥ N√£o foi poss√≠vel conectar ao Ollama em {self.base_url}. Verifique se o app est√° rodando.")

    def get_decision_suggestion(self, case_text: str) -> Optional[str]:
        """Consulta o LLM local para gerar uma minuta."""
        api_url = f"{self.base_url}/api/generate"
        
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
Voc√™ √© um juiz experiente. Analise o caso jur√≠dico abaixo e sugira uma minuta de decis√£o t√©cnica, imparcial e fundamentada. Seja conciso.<|eot_id|><|start_header_id|>user<|end_header_id|>

CASO:
{case_text[:3000]}... (conte√∫do truncado para contexto)

DECIS√ÉO SUGERIDA:<|eot_id|><|start_header_id|>assistant<|end_header_id|>"""

        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.2, # Mais determin√≠stico para decis√µes jur√≠dicas
                "num_ctx": 4096
            }
        }

        try:
            response = requests.post(api_url, json=payload, timeout=120)
            if response.status_code == 200:
                return response.json().get('response', '').strip()
            else:
                logger.error(f"Erro na API Ollama: {response.status_code} - {response.text}")
                return None
        except Exception as e:
            logger.error(f"Falha na requisi√ß√£o ao Ollama: {e}")
            return None

class ETLPipeline:
    def __init__(self, db_url: str):
        self.engine = create_engine(db_url)
        self.vectorizer = VectorizerService()
        self.llm = OllamaService()
        self._setup_db()

    def _setup_db(self):
        """Habilita extens√£o pgvector no banco."""
        try:
            with Session(self.engine) as session:
                session.exec(text("CREATE EXTENSION IF NOT EXISTS vector"))
                session.commit()
            # Cria tabelas se n√£o existirem
            SQLModel.metadata.create_all(self.engine)
            logger.info("‚úÖ Banco de dados configurado e schema atualizado.")
        except Exception as e:
            logger.error(f"Erro cr√≠tico no setup do banco: {e}")
            raise

    def run_batch(self, batch_size=50):
        with Session(self.engine) as session:
            # 1. Busca pendentes
            logger.info("üîç Buscando processos pendentes...")
            statement = select(Processo).where(Processo.processado == False).limit(batch_size)
            batch = session.exec(statement).all()

            if not batch:
                logger.info("zzz Nenhum processo pendente. Dormindo...")
                return False

            ids = [p.id for p in batch]
            logger.info(f"üì¶ Processando Batch IDs: {ids}")

            # 2. Vetoriza√ß√£o (Batch)
            texts = [p.texto_decisao for p in batch]
            embeddings = self.vectorizer.generate_embeddings(texts)

            # 3. Clusteriza√ß√£o (Local Batch)
            # Para clusteriza√ß√£o real, idealmente usar√≠amos um modelo pr√©-treinado ou MiniBatchKmeans persistido.
            # Aqui faremos uma clusteriza√ß√£o simples dentro do lote para identificar grupos imediatos.
            cluster_labels = [0] * len(batch)
            if len(batch) >= 3:
                try:
                    kmeans = KMeans(n_clusters=min(3, len(batch)), n_init='auto')
                    cluster_labels = kmeans.fit_predict(embeddings)
                except Exception as e:
                    logger.warning(f"Erro no clustering: {e}")

            # 4. Atualiza√ß√£o e LLM (Item a Item)
            for i, processo in enumerate(batch):
                try:
                    # Salva vetor
                    processo.embedding = embeddings[i].tolist()
                    processo.cluster_id = int(cluster_labels[i])
                    
                    # Gera Decis√£o (Ollama)
                    logger.info(f"üß† Gerando decis√£o para Processo {processo.numero_processo}...")
                    ai_decision = self.llm.get_decision_suggestion(processo.texto_decisao)
                    
                    if ai_decision:
                        processo.sugestao_ia = ai_decision
                        processo.processado = True
                        processo.data_processamento = time.strftime("%Y-%m-%d %H:%M:%S")
                        session.add(processo)
                    else:
                        logger.warning(f"‚ö†Ô∏è Pulo no processo {processo.id} por falha na IA.")

                except Exception as e:
                    logger.error(f"Erro ao processar item {processo.id}: {e}")

            # Commit do Batch
            session.commit()
            logger.info("‚úÖ Batch processado e salvo no Railway com sucesso!")
            return True

def main():
    if not DATABASE_URL:
        logger.error("‚ùå Configure a vari√°vel de ambiente DATABASE_URL antes de rodar.")
        return

    logger.info("üöÄ Iniciando Legal Tech ETL Worker...")
    pipeline = ETLPipeline(DATABASE_URL)

    try:
        while True:
            has_work = pipeline.run_batch(batch_size=10) # Lotes pequenos para teste
            if not has_work:
                time.sleep(10) # Polling interval
    except KeyboardInterrupt:
        logger.info("üõë Worker interrompido pelo usu√°rio.")

if __name__ == "__main__":
    main()
